\documentclass{../mfa}
\sheet{1}
\begin{document}
\maketitle

\section{}
\subsection{}

\renewcommand{\v}[+1]{\mbf{v}_{#1}}
Wir nehmen an, $\mbf{v}_1$ und $\mbf{v}_2$ seien linear abhängig. Folglich existieren
$\alpha_1,~\alpha_2 \in \mathbb{K}$, sodass $\alpha_1 \mbf{v}_1 + \alpha_2 \mbf{v}_2 = \mbf{0}$
\begin{align}
   \alpha_1 \mbf{v}_1 + \alpha_2 \mbf{v}_2 &= \mbf{0} & \hspace{1cm} & | \cdot A
   \\
   \alpha_1 A \mbf{v}_1 + \alpha_2 A \mbf{v}_2 &= \mbf{0} \\
   \alpha_1 \lambda_1 \mbf{v}_1 + \alpha_2 \lambda_2 \mbf{v}_2 &= \mbf{0} & & | \mbf{v}_i~\text{Eigenvektoren}
\end{align}

Multipliziert man (1) mit $\lambda_1$, so erhält man 
\begin{align}
   \alpha_1 \lambda_1 \mbf{v}_1 + \alpha_2 \lambda_1 \mbf{v}_2 &= \mbf{0} \\
   \intertext{Subtraktion (1) $-$ (6) ergibt}
   \mbf{0} + (\lambda_2 - \lambda_1)\alpha_2 \mbf{v}_2 &= \mbf{0}
\end{align}
Es muss also entweder (a) $\lambda_1 = \lambda_2$ oder (b) $\alpha_2 = 0$ oder
(c) $\mbf{v}_2 = \mbf{0}$ gelten. (a) gilt nicht nach Aufgabenstellung. (c) gilt nicht aufgrund der
Definition eines Eigenvektors. Wenn $\alpha_2 = 0$, muss aber nach Gleichung (1)
Aauch $\alpha_1 = 0$ sein. Daraus folgt, dass $\mbf{v}_1$ und $\mbf{v}_2$ linear unabhängig sein müssen.

\subsection{}

Da $\mbf{v}_1, \mbf{v}_2$ Eigenvektoren sind, gilt 
\begin{equation*}
   A(\alpha_1 \v{1} + \alpha_2 \v{2}) = \alpha_1 A \v{1} + \alpha_2 A \v{2} =
   \alpha_1 \lambda \v{1} + \alpha_2 \lambda \v{2} = \lambda (\alpha_1 \v{1} + \alpha_2 \v{2})
\end{equation*}
Die Linearkombination ist also Eigenvektor von A.

\subsection{}

In (1) wurde gezeigt, dass zwei Eigenvektoren linear unabhängig sind, wenn ihre
Eigenwerte unterschiedlich sind. Wir beweisen zunächst, dass dies für $n$
unterschiedliche Eigenwerte mit zugehörigen Eigenvektoren gilt.

\begin{proof}

\textbf{Induktionsanfang:} Aussage gilt für $n=2$.

\textbf{Induktionsschritt:} Sei bis $n$ bewiesen. Gegeben seien paarweise
verschiedene Eigenwerte $\lambda_1, \lambda_2, \ldots, \lambda_{n+1}$ und
Eigenvektoren $\v{1}, \v{2}, \ldots, \v{n+1}$, wobei $\v{1}, \ldots, \v{n}$
linear unabhängig sind, nach Vorraussetzung. Wir nehmen an, $\v{n+1}$ wäre
linear abhängig von $\v{1}, \ldots, \v{n}$. Es existieren also Skalare
$\alpha_i$ mit 
\setcounter{equation}{0}
\begin{align}
   \alpha_1 \v{1} + \ldots \alpha_n \v{n} + \alpha_{n+1} \v{n+1} &= \mbf{0} \\
   \intertext{Multiplikation mit $A$ liefert}
   \alpha_1 \lambda_1 \v{1} + \ldots \alpha_n \lambda_n \v{n} + \alpha_{n+1} \lambda_{n+1} \v{n+1} &= \mbf{0} \\
   \intertext{denn $\v{i}$ sind Eigenvektoren. Multiplikation von (1) mit
   $\lambda_{n+1}$ ergibt}
   \alpha_1 \lambda_{n+1} \v{1} + \ldots \alpha_n \lambda_{n+1} \v{n} + \alpha_{n+1} \lambda_{n+1} \v{n+1} &= \mbf{0} \\
   \intertext{Subtraktion (2)$-$(3) ergibt}
   \alpha_1(\lambda_1 - \lambda_{n+1})\v{1} + \ldots + \alpha_n (\lambda_n -
   \lambda_{n+1})\v{n} &= \mbf{0}
\end{align}
Da die $\lambda_i$ unterschiedlich sind und $\v{1}, \ldots, \v{n}$ linear
unabhängig, muss $\alpha_1 = \ldots = \alpha_n = 0$ gelten. Mit (1) folgt
$\alpha_{n+1} = 0$ und alle $\v{i}$ sind linear unabhängig. 
\end{proof}

$A$ hat also $n$ linear unabhängige
Eigenvektoren $\mbf{v}_i$. Nach Satz 11.10 ist $A$ als $B=S^{-1} A S$ diagonalisierbar mit
\begin{equation*}
   S = \begin{pmatrix}
      \mbf{v}_1 \ldots, \mbf{v}_n
   \end{pmatrix}
\end{equation*}

   
\end{document}
